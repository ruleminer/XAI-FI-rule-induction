{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from alibi.explainers import AnchorTabular\n",
    "\n",
    "from rmatrix.classification import RMatrixClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itables import show\n",
    "\n",
    "def show_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    length_control: bool = True,\n",
    "    filtering: bool = True,\n",
    "    pagination: bool = True,\n",
    "    show_index: bool = False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Show dataframe as an interactive table using itables library. It is customizable\n",
    "    and allows to filter, sort and paginate the data. Additional options to show method\n",
    "    can be passed using kwargs. When dom option is passed as a kwarg, it will override\n",
    "    config created via length_control, filtering and pagination arguments.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data to be displayed.\n",
    "        length_control (bool, optional): Show length control. Defaults to True.\n",
    "        filtering (bool, optional): Show filtering. Defaults to True.\n",
    "        pagination (bool, optional): Show pagination. Defaults to True.\n",
    "        show_index (bool, optional): Show index. Defaults to False.\"\"\"\n",
    "    pl_language_options = {\n",
    "        \"info\": \"Strona _PAGE_ z _PAGES_\",\n",
    "        \"search\": \"Wyszukaj:\",\n",
    "        \"paginate\": {\n",
    "            \"first\": \"Pierwsza\",\n",
    "            \"last\": \"Ostatnia\",\n",
    "            \"next\": \"Następna\",\n",
    "            \"previous\": \"Poprzednia\",\n",
    "        },\n",
    "        \"lengthMenu\": \"Pokaż _MENU_ wierszy\",\n",
    "    }\n",
    "\n",
    "    if show_index and isinstance(df.index, pd.RangeIndex) and df.index.start == 0:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.index = df.index + 1\n",
    "\n",
    "    # it is assumed that paging is controlled by pagination argument\n",
    "    if \"paging\" in kwargs:\n",
    "        kwargs.pop(\"paging\")\n",
    "\n",
    "    # modify here default \"display nowrap\" of show function to \"display\"\n",
    "    kwargs[\"classes\"] = kwargs[\"classes\"] if \"classes\" in kwargs else \"display\"\n",
    "\n",
    "    if \"dom\" in kwargs:\n",
    "        dom_config = kwargs[\"dom\"]\n",
    "        kwargs.pop(\"dom\")\n",
    "    else:\n",
    "        dom_config = \"tr\" if not filtering else \"trf\"\n",
    "        dom_config += \"l\" if length_control else \"\"\n",
    "        dom_config += \"p\" if pagination else \"\"\n",
    "\n",
    "    show(\n",
    "        df,\n",
    "        language=pl_language_options,\n",
    "        dom=dom_config,\n",
    "        showIndex=show_index,\n",
    "        paging=pagination,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(c) -> float:  # pylint: disable=missing-function-docstring\n",
    "    if (c.p + c.n) == 0:\n",
    "        return 0\n",
    "    return c.p / (c.p + c.n)\n",
    "\n",
    "def coverage(c) -> float:  # pylint: disable=missing-function-docstring\n",
    "    return c.p / c.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cov = 3\n",
    "data_name = \"breast-w\"\n",
    "fi = True\n",
    "file_name = f\"{data_name}_mincov{min_cov}\"\n",
    "generator = RMatrixClassifier(mincov=min_cov, induction_measuer=\"precision\", filter_duplicates=False, filtration=False, cuts_only_between_classes=True, prune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"../data_csv/{data_name}.csv\")\n",
    "data_x = data.drop(columns=[\"class\"])\n",
    "data_y = data[[\"class\"]]\n",
    "\n",
    "ord_tr = OrdinalEncoder()\n",
    "y = ord_tr.fit_transform(data_y)\n",
    "y = np.array([int(obs[0]) for obs in y])\n",
    "\n",
    "class_names = [list(x) for x in ord_tr.categories_][0]\n",
    "feature_names = list(data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n",
    "num_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "data_x_imp_final = []\n",
    "data_x_enc_final = []\n",
    "cat_i = []\n",
    "cat_cat = []\n",
    "cat_i_bool = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    if data_x[feature_names[i]].dtype == \"O\":\n",
    "        data_imp = obj_imputer.fit_transform(data_x[feature_names[i]].values.reshape(-1, 1))\n",
    "        data_imp_flat = [x[0] for x in data_imp]\n",
    "        data_x_imp_final.append(data_imp_flat)\n",
    "        ord = OrdinalEncoder()\n",
    "        data_enc = ord.fit_transform(np.array(data_imp_flat).reshape(-1, 1))\n",
    "        cat_cat.append([list(x) for x in ord.categories_])\n",
    "        cat_i.append(i)\n",
    "        cat_i_bool.append(False)\n",
    "        data_enc_flat = [x[0] for x in data_enc]\n",
    "        data_x_enc_final.append(data_enc_flat)\n",
    "    else:\n",
    "        data_imp = num_imputer.fit_transform(data_x[feature_names[i]].values.reshape(-1, 1))\n",
    "        data_imp_flat = [x[0] for x in data_imp]\n",
    "        data_x_imp_final.append(data_imp_flat)\n",
    "        data_x_enc_final.append(data_imp_flat)\n",
    "        cat_i_bool.append(True)\n",
    "\n",
    "data_x_imp = pd.DataFrame(data_x_imp_final).T\n",
    "data_x_imp.columns = feature_names\n",
    "data_x_enc = pd.DataFrame(data_x_enc_final).T\n",
    "data_x_enc.columns = feature_names\n",
    "data_x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in np.array(feature_names)[cat_i]:\n",
    "    data_x_enc[col] = data_x_enc[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_x_enc, y, test_size=0.2, random_state=42)\n",
    "\n",
    "np.random.seed(0)\n",
    "clf = XGBClassifier(enable_categorical=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame({'vars': feature_names, 'fi': clf.feature_importances_})\n",
    "fi_vars = fi_df.sort_values(\"fi\", ascending=False)[\"vars\"].values\n",
    "fi_df.to_csv(f\"../results_anchor/{file_name}_fi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = clf.predict(X_train)\n",
    "y_test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BACC train: {balanced_accuracy_score(y_train, y_train_preds)}, BACC test: {balanced_accuracy_score(y_test, y_test_preds)}\")\n",
    "print(f\"Kappa train: {cohen_kappa_score(y_train, y_train_preds)}, Kappa test: {cohen_kappa_score(y_test, y_test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_map = [x[0] for x in cat_cat]\n",
    "categories_anchor = dict(zip(cat_i, categories_map))\n",
    "categories_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict_proba(x)\n",
    "explainer = AnchorTabular(predict_fn, feature_names, categories_anchor, seed=42)\n",
    "explainer.fit(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_explainations(data_to_anchor):  \n",
    "\n",
    "    rules = []\n",
    "    rules_len = []\n",
    "    prec = []\n",
    "    cov = []\n",
    "\n",
    "    for idx in tqdm(range(len(data_to_anchor))):\n",
    "\n",
    "        explanation = explainer.explain(data_to_anchor[idx], threshold=0.95)\n",
    "        rules.append(' AND '.join(explanation.anchor))\n",
    "        rules_len.append(len(explanation.anchor))\n",
    "        prec.append(explanation.precision)\n",
    "        cov.append(explanation.coverage)\n",
    "        \n",
    "    anchor_exp_dict = {'anchor_rule': rules, 'anchor_rule_len': rules_len, 'anchor_precision': prec, 'anchor_coverage': cov}    \n",
    "    return pd.DataFrame(anchor_exp_dict)\n",
    "    \n",
    "anchor_train = anchor_explainations(X_train.to_numpy())\n",
    "anchor_test = anchor_explainations(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "NCov = namedtuple('NCov', ['p', 'n', 'P'])\n",
    "\n",
    "def correct_precision_coverage(test, train):\n",
    "\n",
    "    cov_all_list = []\n",
    "    prec_all_list = []\n",
    "\n",
    "    for i in range(len(test)):      \n",
    "        rule_from_anchor = test[\"anchor_rule\"].values[i]\n",
    "        decision = test[\"prediction\"].values[i]\n",
    "        conditions_list = rule_from_anchor.split(\" AND \")\n",
    "        filtered_train = train.copy()\n",
    "        for condition in conditions_list:\n",
    "            key_value = condition.split(\" = \")\n",
    "            filtered_train = filtered_train[filtered_train[key_value[0]] == key_value[1]]\n",
    "        p = len(filtered_train[filtered_train[\"prediction\"]==decision]) + 1\n",
    "        n = len(filtered_train[filtered_train[\"prediction\"]!=decision])\n",
    "        P = len(train[train[\"prediction\"]==decision]) + 1\n",
    "        new_cov_loc = NCov(p, n, P)\n",
    "        cov_all_list.append(coverage(new_cov_loc))\n",
    "        prec_all_list.append(precision(new_cov_loc))\n",
    "\n",
    "    return prec_all_list, cov_all_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_org = data_x_imp.iloc[X_train.index,:].reset_index(drop=True)\n",
    "y_train_pred_org = pd.Series(ord_tr.inverse_transform(y_train_preds.reshape(-1, 1)).flatten(), name=\"prediction\")\n",
    "y_train_org = pd.Series(ord_tr.inverse_transform(y_train.reshape(-1, 1)).flatten(), name=\"class\")\n",
    "X_test_org = data_x_imp.iloc[X_test.index,:].reset_index(drop=True)\n",
    "X_test_org.columns = feature_names\n",
    "y_test_pred_org = pd.Series(ord_tr.inverse_transform(y_test_preds.reshape(-1, 1)).flatten(), name=\"prediction\")\n",
    "y_test_org = pd.Series(ord_tr.inverse_transform(y_test.reshape(-1, 1)).flatten(), name=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in np.array(feature_names)[cat_i_bool]:\n",
    "    X_train_org[col] = X_train_org[col].astype(\"float64\")\n",
    "    X_test_org[col] = X_test_org[col].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmatrix_explainations(x, y):  \n",
    "\n",
    "    rules = []\n",
    "    rules_len = []\n",
    "    prec = []\n",
    "    cov_list = []\n",
    "\n",
    "    for idx in tqdm(range(len(x))):\n",
    "\n",
    "        if fi:\n",
    "            rule, cov = generator.local_explanation(example_X=x.iloc[idx:idx+1], example_y=y.iloc[idx:idx+1], X=X_train_org, y=y_train_org, attributes=fi_vars)\n",
    "        else:\n",
    "            rule, cov = generator.local_explanation(example_X=x.iloc[idx:idx+1], example_y=y.iloc[idx:idx+1], X=X_train_org, y=y_train_org, attributes=[])\n",
    "        rules.append(rule.premise.to_string(feature_names))\n",
    "        rules_len.append(len(rule.premise.to_string(feature_names).split(\"AND\")))\n",
    "        prec.append(precision(cov))\n",
    "        cov_list.append(coverage(cov))\n",
    "        \n",
    "    rmatrix_exp_dict = {'rmatrix_rule': rules, 'rmatrix_rule_len': rules_len, 'rmatrix_precision': prec, 'rmatrix_coverage': cov_list}\n",
    "        \n",
    "    return pd.DataFrame(rmatrix_exp_dict)\n",
    "\n",
    "rmatrix_train = rmatrix_explainations(X_train_org, y_train_pred_org)\n",
    "rmatrix_test = rmatrix_explainations(X_test_org, y_test_pred_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmatrix_unique_rules(rule_str):\n",
    "    features = []\n",
    "    conditions = rule_str.split(\" AND \")\n",
    "    for condition in conditions:\n",
    "        feature_value = condition.split(\" = \")\n",
    "        features.append(feature_value[0])\n",
    "    \n",
    "    return len(np.unique(features))\n",
    "\n",
    "rmatrix_train[\"rmatrix_rule_len_unique\"] = rmatrix_train[\"rmatrix_rule\"].apply(rmatrix_unique_rules)\n",
    "rmatrix_test[\"rmatrix_rule_len_unique\"] = rmatrix_test[\"rmatrix_rule\"].apply(rmatrix_unique_rules)\n",
    "rmatrix_train = rmatrix_train[['rmatrix_rule', 'rmatrix_rule_len', 'rmatrix_rule_len_unique', 'rmatrix_precision', 'rmatrix_coverage']]\n",
    "rmatrix_test = rmatrix_test[['rmatrix_rule', 'rmatrix_rule_len', 'rmatrix_rule_len_unique', 'rmatrix_precision', 'rmatrix_coverage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([X_train_org, y_train_org, y_train_pred_org, rmatrix_train, anchor_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_len = train_all.groupby([\"rmatrix_rule_len\", \"anchor_rule_len\"])[\"rmatrix_rule_len\", \"anchor_rule_len\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False)\n",
    "show_dataframe(rule_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statystyki opisowe długości reguł, precision oraz coverage. Dla anchor przeliczono miary precision i coverage zgodnie ze wzorami z rmatrix (przyrostek correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "NCov = namedtuple('NCov', ['p', 'n', 'P'])\n",
    "\n",
    "def correct_precision_coverage(test, train):\n",
    "\n",
    "    cov_all_list = []\n",
    "    prec_all_list = []\n",
    "\n",
    "    for i in range(len(test)):      \n",
    "        rule_from_anchor = test[\"anchor_rule\"].values[i]\n",
    "        decision = test[\"prediction\"].values[i]\n",
    "        conditions_list = rule_from_anchor.split(\" AND \")\n",
    "        filtered_train = train.copy()\n",
    "        for condition in conditions_list:\n",
    "            key_value = condition.split(\" \")\n",
    "            if key_value[1] == \"=\":\n",
    "                if key_value[0] in np.array(feature_names)[cat_i_bool]:\n",
    "                    filtered_train = filtered_train[filtered_train[key_value[0]] == float(key_value[2])]\n",
    "                else:\n",
    "                    filtered_train = filtered_train[filtered_train[key_value[0]] == key_value[2]]\n",
    "            elif len(key_value) > 3:\n",
    "                filtered_train = filtered_train.query(condition)\n",
    "            else:\n",
    "                if key_value[0] in np.array(feature_names)[cat_i_bool]:\n",
    "                    filtered_train = filtered_train.query(condition)\n",
    "                else:\n",
    "                    filtered_train = filtered_train.query(f\"{key_value[0]}{key_value[1]}'{key_value[0]}'\")\n",
    "        p = len(filtered_train[filtered_train[\"prediction\"]==decision]) + 1\n",
    "        n = len(filtered_train[filtered_train[\"prediction\"]!=decision])\n",
    "        P = len(train[train[\"prediction\"]==decision]) + 1\n",
    "        new_cov_loc = NCov(p, n, P)\n",
    "        cov_all_list.append(coverage(new_cov_loc))\n",
    "        prec_all_list.append(precision(new_cov_loc))\n",
    "\n",
    "    return prec_all_list, cov_all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_precision_correct, anchor_coverage_correct = correct_precision_coverage(train_all, train_all)\n",
    "train_all[\"anchor_precision_correct\"] = anchor_precision_correct\n",
    "train_all[\"anchor_coverage_correct\"] = anchor_coverage_correct\n",
    "train_all.to_csv(f\"results_anchor/{file_name}_train.csv\", index=False, sep=\";\")\n",
    "train_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar precision dla przypadków ze zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_all, x=\"rmatrix_precision\", y=\"anchor_precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar precision dla przypadków ze zbioru treningowego - korekta miary w anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_all, x=\"rmatrix_precision\", y=\"anchor_precision_correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar coverage dla przypadków ze zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_all, x=\"rmatrix_coverage\", y=\"anchor_coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar coverage dla przypadków ze zbioru treningowego - korekta miary w anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_all, x=\"rmatrix_coverage\", y=\"anchor_coverage_correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all[\"anchor_rule_all\"] = train_all[\"anchor_rule\"] + \" THEN \" + train_all[\"prediction\"]\n",
    "train_all[\"rmatrix_rule_all\"] = train_all[\"rmatrix_rule\"] + \" THEN \" + train_all[\"prediction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba unikalnych reguł anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_all.groupby([\"anchor_rule_all\"])[\"anchor_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęściej występujące lokalne objaśnienia wygenerowane przez anchor w zbiorze treningowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe(train_all.groupby([\"anchor_rule_all\"])[\"anchor_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba unikalnych reguł RMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_all.groupby([\"rmatrix_rule_all\"])[\"rmatrix_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęściej występujące lokalne objaśnienia wygenerowane przez rmatrix w zbiorze treningowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe(train_all.groupby([\"rmatrix_rule_all\"])[\"rmatrix_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = pd.concat([X_test_org, y_test_org, y_test_pred_org, rmatrix_test, anchor_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie długości reguł rmatrix i anchor oraz liczba przypadków w zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_len = test_all.groupby([\"rmatrix_rule_len\", \"anchor_rule_len\"])[\"rmatrix_rule_len\", \"anchor_rule_len\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False)\n",
    "show_dataframe(rule_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statystyki opisowe długości reguł, precision oraz coverage. Dla anchor przeliczono miary precision i coverage zgodnie ze wzorami z rmatrix (przyrostek correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_precision_correct, anchor_coverage_correct = correct_precision_coverage(test_all, train_all)\n",
    "test_all[\"anchor_precision_correct\"] = anchor_precision_correct\n",
    "test_all[\"anchor_coverage_correct\"] = anchor_coverage_correct\n",
    "test_all.to_csv(f\"results_anchor/{file_name}_test.csv\", index=False, sep=\";\")\n",
    "test_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar precision dla przypadków ze zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=test_all, x=\"rmatrix_precision\", y=\"anchor_precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar precision dla przypadków ze zbioru testowego - korekta miary w anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=test_all, x=\"rmatrix_precision\", y=\"anchor_precision_correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar coverage dla przypadków ze zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=test_all, x=\"rmatrix_coverage\", y=\"anchor_coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie miar coverage dla przypadków ze zbioru testowego - korekta miary w anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=test_all, x=\"rmatrix_coverage\", y=\"anchor_coverage_correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all[\"anchor_rule_all\"] = test_all[\"anchor_rule\"] + \" THEN \" + test_all[\"prediction\"]\n",
    "test_all[\"rmatrix_rule_all\"] = test_all[\"rmatrix_rule\"] + \" THEN \" + test_all[\"prediction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba unikalnych reguł anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_all.groupby([\"anchor_rule_all\"])[\"anchor_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęściej występujące lokalne objaśnienia wygenerowane przez anchor w zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe(test_all.groupby([\"anchor_rule_all\"])[\"anchor_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczba unikalnych reguł RMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_all.groupby([\"rmatrix_rule_all\"])[\"rmatrix_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęściej występujące lokalne objaśnienia wygenerowane przez rmatrix w zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe(test_all.groupby([\"rmatrix_rule_all\"])[\"rmatrix_rule_all\"].size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
